
from celery import shared_task
import time
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
import warnings
import logging
 


@shared_task(bind=True)
def train_model_task(self,X_train, y_train, model_name="random_forest", params=None, use_grid_search=False, search_models=False, scoring="accuracy", cv=5,models = None,param_grid= None):
    """
    Entra√Æne un mod√®le de Machine Learning avec ou sans GridSearch, en g√©rant les erreurs potentielles.

    :param X_train: Features d'entra√Ænement
    :param y_train: Labels d'entra√Ænement
    :param model_name: Nom du mod√®le √† utiliser (si search_models=False)
    :param params: Dictionnaire des param√®tres si non None et use_grid_search=False
    :param use_grid_search: Si True, utilise GridSearch pour chercher les meilleurs hyperparam√®tres
    :param search_models: Si True, GridSearch explore aussi plusieurs mod√®les
    :param scoring: M√©trique de scoring pour GridSearch (ex: "accuracy", "f1", "roc_auc", etc.)
    :param cv: Nombre de folds pour la validation crois√©e
    :return: Mod√®le entra√Æn√©, meilleurs param√®tres trouv√©s (ou ceux par d√©faut), message d'information
    """
    logging.info("üöÄ D√©but de l'entra√Ænement du mod√®le...")
    print("modele_nameintofonction",model_name)
    if params:
        for param, value in params.items():
            # Si la valeur est une cha√Æne qui repr√©sente un nombre, on la convertit
            if value.replace('.', '', 1).isdigit():  # Si la valeur peut √™tre un nombre
                params[param] = float(value) if '.' in value else int(value)
    

    

    # Variables pour stocker le meilleur mod√®le en cas de recherche multiple
    best_model = None
    best_score = float('-inf')
    best_params = None
    models_tested = 0  

    # D√©sactiver les warnings inutiles
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")

        # Recherche du meilleur mod√®le parmi plusieurs algorithmes
        if search_models:
            logging.info("üîç Recherche du meilleur mod√®le parmi plusieurs algorithmes...")
            
            for model_key, model in models.items():
                try:
                    logging.info(f"üîç Optimisation du mod√®le: {model_key} avec scoring={scoring} et cv={cv}")
                    
                    grid_search = GridSearchCV(model, param_grid[model_key], cv=cv, scoring=scoring, refit='accuracy')
                    grid_search.fit(X_train, y_train)
                    
                    models_tested += 1  # Compter les mod√®les test√©s avec succ√®s

                    if grid_search.best_score_ > best_score:
                        best_score = grid_search.best_score_
                        best_model = grid_search.best_estimator_
                        best_params = grid_search.best_params_

                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è Erreur avec le mod√®le {model_key}: {e}")

            if best_model is None:
                return None, None, "‚ùå Aucun mod√®le n'a r√©ussi √† s'entra√Æner."
            else:
                print("Meilleurs scores pour chaque m√©trique :")
                for metric in scoring:
                    print(f"{metric}: {grid_search.cv_results_['mean_test_' + metric].max()}")
                return best_model, best_params, f"‚úÖ Meilleur mod√®le trouv√©: {best_model} avec score={best_score}"

        # Entra√Ænement d'un seul mod√®le sp√©cifique
        else:
            model = models[model_name]

            if model is None:
                return None, None, f"‚ùå Mod√®le '{model_name}' non reconnu. Choisissez parmi {list(models.keys())}."

            try:
                # Si GridSearch est activ√©, recherche des meilleurs hyperparam√®tres
                if use_grid_search:
                    logging.info(f"üîç Recherche des meilleurs param√®tres pour {model_name}...")
                    grid_search = GridSearchCV(model, param_grid[model_name], cv=cv, scoring=scoring, refit='accuracy')
                    grid_search.fit(X_train, y_train)

                    print("Meilleurs scores pour chaque m√©trique :")
                    for metric in scoring:
                        print(f"{metric}: {grid_search.cv_results_['mean_test_' + metric].max()}")
                    return grid_search.best_estimator_, grid_search.best_params_, f"‚úÖ Mod√®le optimis√©: {model_name}"

                # Si des param√®tres sont fournis, les appliquer au mod√®le
                elif params:
                    model.set_params(**params)

                # Entra√Ænement du mod√®le
                logging.info(f"üöÄ Entra√Ænement du mod√®le {model_name}...")
                model.fit(X_train, y_train)

                # Retourner les param√®tres d√©finis ou ceux par d√©faut
                return model, params if params else model.get_params(), f"‚úÖ Mod√®le entra√Æn√©: {model_name}"

            except Exception as e:
                return None, None, f"‚ö†Ô∏è Erreur avec {model_name}: {e}"
